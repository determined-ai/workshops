name: tiny llama deepspeed easy WarmupDecayLR batch size 2
debug: false
workspace: agnieszka
project: llm-blog2
environment:
  environment_variables:
    - NCCL_DEBUG=INFO
    - HF_HOME=/nvmefs1/agnieszka.ciborowska/hf_cache
  image: determinedai/environments-dev:python-3.10-pytorch-2.0-deepspeed-0.10.0-smartsim
resources:
  slots_per_trial: 2
  resource_pool: A100
searcher:
  name: single
  max_length:
    batches: 5000
  metric: eval_accuracy
  smaller_is_better: false
hyperparameters:
  #model: "mistralai/Mistral-7B-Instruct-v0.2"
  model: "TinyLlama/TinyLlama-1.1B-Chat-v0.4"
  dataset_subset: "easy"
  lora: false
  training_args:
    output_dir: "/tmp/llm_finetuning"
    max_steps: 5000
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 4
    fp16: true
    evaluation_strategy: "steps"
    eval_steps: 1000
    logging_strategy: "steps"
    logging_steps: 10
    save_strategy: "steps"
    save_steps: 5000
    learning_rate: 1e-5
    deepspeed: "ds_configs/ds_config_stage_3.json"
entrypoint: >-
  python -m determined.launch.deepspeed
  python finetune.py
max_restarts: 0