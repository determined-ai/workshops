{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c0c5b6-0435-44d9-aedf-85adf923eb59",
   "metadata": {},
   "source": [
    "<img src=\"imgs/hpe_logo.png\" alt=\"HPE Logo\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c37a8-f4b6-45a3-ac61-cc8a0ac2962d",
   "metadata": {},
   "source": [
    "# Chicago DS Summit Workshop: Chatbot tutorial: Finetuning GPT models at Scale with Machine Learning Development Environment\n",
    " ----\n",
    "\n",
    "Note this Demo is based on https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "\n",
    "## Objective: Train your own chatbot\n",
    "This notebook walks you through finetuning your own chatbot.\n",
    "We will learn what are Generative Pretrained Transformers (GPT), how to finetune to your own domain, and finetune at Scale\n",
    "\n",
    "## Why is this exciting: The rise of generative language modeling\n",
    "Generative Language models like GPT-4 and ChatGPT enable exciting applications that were not possible before!\n",
    "\n",
    "* `Enterprise`: Chatbots for helpdesk support\n",
    "* `Healthcare`: Chatbots for scheduling appts, manage coverage, process claims\n",
    "* `Manufacturing`: Chatbots for checking supplies and inventory check\n",
    "* `Financial Services`: Chatbots for investment and account support\n",
    "\n",
    "With Machine Learning Development Environment (MLDE, based on DeterminedAI) we help engineers create powerful language modeling application at scale!\n",
    "\n",
    "## Why MLDE and DeterminedAI\n",
    "\n",
    "Developing robust, high performing Deep Learning application is challenging. Succesful team requires data, compute, and great infrastructure. Building and managing distributed training, automatic checkpointing, hyperparameter search and metrics tracking is critical. \n",
    "\n",
    "MLDE can remove the burden of writing and maintaining a custom training harness and offers a streamlined approach to onboard new models to a state-of-the-art training platform, offering the following integrated platform features:\n",
    "\n",
    "<img src=\"imgs/det_components.jpg\" alt=\"Determined Components\" width=\"900\">\n",
    "\n",
    "Determined provides a high-level framework APIs for PyTorch, Keras, and Estimators that let users describe their model without boilerplate code. Determined reduces boilerplate by providing a state-of-the-art training loop that provides distributed training, hyperparameter search, automatic mixed precision, reproducibility, and many more features.\n",
    "\n",
    "<h3>Overview of this workshop</h3>\n",
    "\n",
    "## Overview of Tutorial\n",
    "\n",
    "* Intro to GPT models\n",
    "* Data Science Chatbot Baseline: Chatbot to ask Data Science questions\n",
    "* Finetune GPT model on a Data Science book to better answer Data Science Tasks\n",
    "* Overview of integrating Pytorch training code into MLDE\n",
    "* English to Latex Chatbot: Convert english to latex\n",
    "* Finetune GPT model on dataset to convert english to latex\n",
    "* User exercise: Finetune bot on custom text data\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c271e3-0744-4c54-a4b9-dcd5dd9a4ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, GPT2LMHeadModel, pipeline, \\\n",
    "                         Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from determined.experimental import Determined\n",
    "from utils import load_model_from_checkpoint\n",
    "from determined.experimental import client\n",
    "from determined import pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c557e7d-3d7c-4629-9b0a-efa9b1effbd5",
   "metadata": {},
   "source": [
    "# What is GPT\n",
    "\n",
    "<img src=\"imgs/openAI-gpt2.png\" alt=\"Determined Components\" width=\"900\">\n",
    "\n",
    "GPT2 is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.\n",
    "\n",
    "Other models available: GPT2-Medium, GPT2-Large and GPT2-XL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a31e5b-8147-424f-8a5c-0c1367f47c6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Science Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b7537-1345-43d3-bef9-420a6b9266c5",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "We will load a pretrained model on X dataset and see how it responds to data science questions.\n",
    "\n",
    "Prompt: `A test statistic is a value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd68c9-e820-4872-8577-3a3ec23024d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 49.7MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 33.8MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 249kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 548M/548M [00:03<00:00, 175MB/s]  \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 10.9kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 109MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # load up a GPT2 model\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', model=model, tokenizer='gpt2',\n",
    "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3cbc58-8870-4bf3-a39a-d86b868cab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='A test statistic is a value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f56d2-6208-4a7e-ab34-c16b41e2698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "A test statistic is a value that shows a change in a certain probability value.\n",
      "\n",
      "To put it simply, if this statistic changes, then the test statistic gets a change of 0.0 from the last time you saw it. Then when you\n",
      "----------\n",
      "A test statistic is a value that makes little any sense.\n",
      "1.1 Test Number Test Number to be verified\n",
      "\n",
      "\n",
      "The results from this test match the 1st Test Average. The 1st Test Average matches that the 100 second average was at\n",
      "----------\n",
      "A test statistic is a value between 1 and 50. This test statistic can be checked at any time by simply viewing the sample, and the value is then multiplied by 50.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('----------')\n",
    "for generated_sequence in pretrained_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034676f4-6de4-46f9-a36a-786acb6ad2e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What model training looks like without Determined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d945b-5674-4358-8ddb-fcdf7dd54ca1",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./data/PDS2.txt',  # Principles of Data Science - Sinan Ozdemir\n",
    "    block_size=32  # length of each chunk of text to use as a datapoint\n",
    ")\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "\n",
    "train_sampler = RandomSampler(dataset)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_dataloader = DataLoader(dataset, collate_fn =data_collator ,sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "t_total = len(dataset) // gradient_accumulation_steps * num_train_epochs\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.zero_grad()\n",
    "train_iterator = trange(int(num_train_epochs), desc=\"Epoch\", disable=local_rank not in [-1, 0])\n",
    "set_seed(0)\n",
    "for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=local_rank not in [-1, 0])\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        inputs, labels = mask_tokens(batch, tokenizer, None) if mlm else (batch, batch)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        model.train()\n",
    "\n",
    "        outputs = model(inputs, masked_lm_labels=labels) if mlm else model(inputs, labels=labels)\n",
    "        loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "        if fp16:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd299317-233a-4c5f-97b1-b033485736ec",
   "metadata": {},
   "source": [
    "### Here is what it would look like to do hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392e2f2-eb61-4524-bc87-0d100ded0d1c",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def train(lr,m):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path='./data/PDS2.txt',  # Principles of Data Science - Sinan Ozdemir\n",
    "        block_size=32  # length of each chunk of text to use as a datapoint\n",
    "    )\n",
    "    config = GPT2Config.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model.to(device)\n",
    "    train_sampler = RandomSampler(dataset)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    train_dataloader = DataLoader(dataset, collate_fn =data_collator ,sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "    t_total = len(dataset) // gradient_accumulation_steps * num_train_epochs\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(num_train_epochs), desc=\"Epoch\", disable=local_rank not in [-1, 0])\n",
    "    set_seed(0)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            inputs, labels = mask_tokens(batch, tokenizer, None) if mlm else (batch, batch)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "\n",
    "            outputs = model(inputs, masked_lm_labels=labels) if mlm else model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "            if fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            loss.backward()\n",
    "    model, loss\n",
    "def hp_grid_search():\n",
    "    for lr in np.logspace(-4, -2, num=10):\n",
    "        for m in np.linspace(0.7, 0.95, num=10):\n",
    "            print(f\"Training model with learning rate {lr} and momentum {m}\")\n",
    "            model, loss = train(lr,m)\n",
    "            print(f\"Train Loss: {loss}\\n\")\n",
    "\n",
    "try:\n",
    "    hp_grid_search()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600006b-faae-4719-9e91-565e5db8e2e9",
   "metadata": {},
   "source": [
    "#### What's Missing?\n",
    "This approach works in theory -- we could get a good model, save it, and use it for predictions. But we're missing a lot from the ideal state:\n",
    "\n",
    "#### Distributed training\n",
    "    - Parallel search\n",
    "    - Intelligent checkpointing\n",
    "    - Interruptibility and fault tolerance\n",
    "    - Logging of experiment configurations and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2484ebd-c5dd-41c8-b80c-56bc7e5a48fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overview of integrating Pytorch training code into MLDE\n",
    "\n",
    "The main components for any deep learning training loop are the following:\n",
    "* Datasets\n",
    "* Dataloader\n",
    "* Model\n",
    "* Optimizer\n",
    "* (Optional) Learn rate schedule\n",
    "* training a batch, evaluating a batch\n",
    "\n",
    "We will show how to integrate each core part into MLDE using the PyTorchTrial API. Note we have another API called CoreAPI, that supports flexibility if your team wants to integrate more complex Machine Learning codebases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951811f1-d9bf-4750-8599-4e9089919121",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Template Class that integrates DL code with MLDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afbaa0-0c2f-48be-a702-aef394a7670a",
   "metadata": {},
   "source": [
    "```python\n",
    "import filelock\n",
    "import os\n",
    "from typing import Any, Dict, Sequence, Tuple, Union, cast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from determined.pytorch import DataLoader, PyTorchTrial, PyTorchTrialContext\n",
    "\n",
    "import data\n",
    "\n",
    "TorchData = Union[Dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor]\n",
    "\n",
    "class GPT2Trial(PyTorchTrial):\n",
    "    def __init__(self, context: PyTorchTrialContext) -> None:\n",
    "        # Trial context contains info about the trial, such as the hyperparameters for training\n",
    "        self.context = context\n",
    "        \n",
    "        # init and wrap model, optimizer, LRscheduler, datasets\n",
    "       \n",
    "\n",
    "    def build_training_data_loader(self) -> DataLoader:\n",
    "        # create train dataloader from dataset\n",
    "        return DataLoader()\n",
    "\n",
    "    def build_validation_data_loader(self) -> DataLoader:\n",
    "        # create train dataloader from dataset\n",
    "        return DataLoader()\n",
    "\n",
    "    def train_batch(self, batch: TorchData, epoch_idx: int, batch_idx: int)  -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def evaluate_batch(self, batch: TorchData) -> Dict[str, Any]:\n",
    "        return {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182d059-603a-40ef-9d04-eab7d70eb884",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wrap Model\n",
    "* Wrapping model to the TrialContext allows MLDE to reduces boilerplate code\n",
    "* Providing a state-of-the-art training loop that provides distributed training, hyperparameter search, automatic mixed precision, reproducibility, and many more features\n",
    "* All the models, optimizers, and LR schedulers must be wrapped with wrap_model and wrap_optimizer respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7c6b2-1b6a-4347-a552-cfeb42169bd0",
   "metadata": {},
   "source": [
    "```python\n",
    "self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Wrapping model to the TrialContext \n",
    "self.model = self.context.wrap_model(self.model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1477398-8e18-4056-b1ce-398e48cd1d51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wrap Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4051af-bc83-43bb-acb0-b5529d402fe8",
   "metadata": {},
   "source": [
    "```python\n",
    "self.optimizer = self.context.wrap_optimizer(\n",
    "            AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.adam_epsilon)\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92578625-94e2-47eb-8304-aee0a00be52c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wrap LR scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a5c51-d70c-46be-b976-4e5946c2c021",
   "metadata": {},
   "source": [
    "```python\n",
    "self.scheduler = self.context.wrap_lr_scheduler(\n",
    "    get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=self.warmup_steps,\n",
    "                                    num_training_steps=self.t_total),\n",
    "    LRScheduler.StepMode.MANUAL_STEP\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd085b7-6a66-4aeb-b1b6-cba5cc3bb126",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Integrate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf815c-2c3e-44a4-bc1a-dfb84eee703b",
   "metadata": {},
   "source": [
    "```python\n",
    "dataset = TextDataset(\n",
    "                tokenizer=tokenizer,\n",
    "                file_path='/run/determined/workdir/shared_fs/workshop_data/hamlet.txt',\n",
    "                block_size=32  # length of each chunk of text to use as a datapoint\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed797aff-ce43-4492-8810-a1295a2c9f60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Implement Train Dataloader and Validation Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0b04b-b4ab-4eb0-85e0-9f302b2149ae",
   "metadata": {},
   "source": [
    "```python\n",
    "def build_training_data_loader(self) -> None:\n",
    "    '''\n",
    "    '''\n",
    "    self.train_sampler = RandomSampler(self.dataset)\n",
    "    self.train_dataloader = DataLoader(self.dataset, collate_fn =self.data_collator ,sampler=self.train_sampler, batch_size=self.train_batch_size)\n",
    "    return self.train_dataloader\n",
    "def build_validation_data_loader(self) -> None:\n",
    "    '''\n",
    "    '''\n",
    "    self.eval_sampler = SequentialSampler(self.dataset)\n",
    "    self.validataion_dataloader = DataLoader(self.dataset,collate_fn =self.data_collator, sampler=self.eval_sampler, batch_size=self.eval_batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20951d8-a14f-45cc-81e2-2b39b0b3f608",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Implement Train Batch and Evaluate Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ed9c9-7be5-47a8-a8c4-5f3c0b30e869",
   "metadata": {},
   "source": [
    "```python\n",
    "def train_batch(self,batch,epoch_idx, batch_idx):\n",
    "    '''\n",
    "    '''\n",
    "    inputs,labels = self.format_batch(batch)\n",
    "    outputs = self.model(inputs, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    train_result = {\n",
    "        'loss': loss\n",
    "    }\n",
    "    self.context.backward(train_result[\"loss\"])\n",
    "    self.context.step_optimizer(self.optimizer)\n",
    "    return train_result\n",
    "\n",
    "def evaluate_batch(self,batch):\n",
    "    '''\n",
    "    '''\n",
    "    inputs,labels = self.format_batch(batch)\n",
    "    outputs = self.model(inputs, labels=labels)\n",
    "    lm_loss = outputs[0]\n",
    "    eval_loss = lm_loss.mean().item()\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    results = {\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b0555-5cea-460d-b1e4-5b84ecc8180d",
   "metadata": {},
   "source": [
    "# Defining Training Experiment with Model Config\n",
    "In Determined, a trial is a training task that consists of a dataset, a deep learning model, and values for all of the model’s hyperparameters. An experiment is a collection of one or more trials: an experiment can either train a single model (with a single trial), or can define a search over a user-defined hyperparameter space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7016d230-c108-4cff-bbe6-be1caefd1b08",
   "metadata": {},
   "source": [
    "Here is what a configuration file looks like for a single trial experiment\n",
    "```yaml\n",
    "name: gpt2_finetune_data_science_chatbot\n",
    "workspace: <your_workspace>\n",
    "project: <your_project>\n",
    "description: \"Chicago DS Workshop\"\n",
    "hyperparameters:\n",
    "    global_batch_size: 32\n",
    "    weight_decay: 0.0\n",
    "    learning_rate: 5e-5\n",
    "    adam_epsilon: 1e-8\n",
    "    warmup_steps: 0\n",
    "    epochs: 10\n",
    "    gradient_accumulation_steps: 1\n",
    "    dataset_name: 'PDS2'\n",
    "environment:\n",
    "    image: \"hugcyrill/workshops:chat_0.1\"\n",
    "records_per_epoch: 147 # 4696 examples, 128 per batch: 601088/21 is 147 records for an epoch\n",
    "resources:\n",
    "    slots_per_trial: 1\n",
    "min_validation_period:\n",
    "  batches: 4\n",
    "min_checkpoint_period:\n",
    "  batches: 147\n",
    "searcher:\n",
    "    name: single\n",
    "    metric: eval_loss\n",
    "    max_length:\n",
    "        epochs: 30\n",
    "    smaller_is_better: true\n",
    "max_restarts: 0\n",
    "entrypoint: model_def:GPT2Finetune\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f523bbf-8a06-4ca8-8d4b-21760f60fd70",
   "metadata": {},
   "source": [
    "Here is what a configuration yaml file looks like to do a hyperparameter search\n",
    "```yaml\n",
    "name: adaptive_gpt2_finetune_data_science_chatbot\n",
    "workspace: <your_workspace>\n",
    "project: <your_project>\n",
    "description: \"Chicago DS Workshop\"\n",
    "hyperparameters:\n",
    "    global_batch_size: 32\n",
    "    weight_decay: 0.0\n",
    "    learning_rate:\n",
    "        type: log\n",
    "        minval: -6.0\n",
    "        maxval: -4.0\n",
    "        base: 10.0\n",
    "    adam_epsilon:\n",
    "        type: log\n",
    "        minval: -10.0\n",
    "        maxval: -4.0\n",
    "        base: 10.0\n",
    "    warmup_steps: 0\n",
    "    epochs: 10\n",
    "    gradient_accumulation_steps: 1\n",
    "    dataset_name: 'PDS2'\n",
    "environment:\n",
    "    image: \"hugcyrill/workshops:chat_0.1\"\n",
    "records_per_epoch: 147 # 4696 examples, 32 per batch: 4696/21 is 147 records for an epoch\n",
    "resources:\n",
    "    slots_per_trial: 1\n",
    "min_validation_period:\n",
    "  batches: 4\n",
    "min_checkpoint_period:\n",
    "  batches: 147\n",
    "searcher:\n",
    "    name: adaptive_asha\n",
    "    metric: eval_loss\n",
    "    max_length:\n",
    "        epochs: 30\n",
    "    smaller_is_better: true\n",
    "    max_trials: 4\n",
    "max_restarts: 0\n",
    "entrypoint: model_def:GPT2Finetune\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43e3c7-06e3-491b-a505-bdf9f959b96c",
   "metadata": {},
   "source": [
    "# User Task: Update `const_ds_chatbot.yaml` and `const_eng_to_latex.yaml` configs\n",
    "Then, please  the `const_ds_chatbot.yaml` file in the `determined_files/` folder and look for the workspace and project fields. Please replace the placeholders with your workspace name and project name, which you created during the preparation session:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa3bb8-2112-42ae-af95-53ba8a2f38f1",
   "metadata": {},
   "source": [
    "```yaml\n",
    "workspace: <your_workspace>\n",
    "project: <your_project>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffafc3d-7a7a-45d3-b5e9-31a4f31799a6",
   "metadata": {},
   "source": [
    "Update the `const_eng_to_latex.yaml` file in the `determined_files/`in the same manner.  replace the placeholders with your workspace name and project name,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86215-287d-47b8-8f16-d9942fd919cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning Chatbot Part 1: Data Science Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833d407a-0cd3-44b3-a909-b44f00d7ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Preparing files to send to master... 10.5KB and 8 files\n",
      "Created experiment 59\n"
     ]
    }
   ],
   "source": [
    "!det experiment create \\\n",
    "    determined_files/const_ds_chatbot.yaml \\\n",
    "    determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32982e-834b-404d-ba3e-e6adc3bff3e5",
   "metadata": {},
   "source": [
    "### See Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c54ab72-b109-4b9b-a877-ab6d4ca82435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6a01f70b-55c3-4e10-874c-6684d8425b20\n"
     ]
    }
   ],
   "source": [
    "experiment_id = 66\n",
    "MODEL_NAME = \"gpt2\"\n",
    "checkpoint = client.get_experiment(experiment_id).top_checkpoint(sort_by=\"eval_loss\", smaller_is_better=True)\n",
    "print(checkpoint.uuid)\n",
    "loaded_model = load_model_from_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e2582a-50fd-43f6-a1d3-efd7fcfced6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finetuned_generator = pipeline(\n",
    "    'text-generation', model=loaded_model, tokenizer=tokenizer,\n",
    "    config={'max_length': 200,  'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a094cab-8b6c-4117-b08c-eb6066e394ba",
   "metadata": {},
   "source": [
    "Here is how the chatbot responds to same prompt after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd448dc-f45a-4d29-8405-93c75721e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='A test statistic is a value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624d94e5-11f1-4bd7-9518-c59eeac7c1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "A test statistic is a value that represents (what) the test-takers used to be. Let's get an idea for the types of data\n",
      "data.\n",
      "\n",
      "We should be aware that the best way to understand the data that the test\n",
      "----------\n",
      "A test statistic is a value at the 0% level and refers to an estimate made that is 1/10th or 0.01%\n",
      "This is the\n",
      "average level difference between the two levels within the same sample\n",
      "The level of significance\n",
      "\n",
      "----------\n",
      "A test statistic is a value that can be seen when plotting the two data points with each other. If we want to work through the differences between our tests, we can\n",
      "scatter through data,\n",
      "\n",
      "and compare them.\n",
      "\n",
      "[ 214\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('----------')\n",
    "for generated_sequence in finetuned_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e528c-a9db-4f28-a4d5-37115a39860d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning Experiment number 2: # English 2 Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be3455-aec5-43de-ba08-186e3c1386ca",
   "metadata": {},
   "source": [
    "## Baseline english to latex chatbot\n",
    "Here we are downloading pretrained weights and seeing how GPT does on a question to convert an english description into latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce5f14a9-2ce1-4453-94a4-fd9a5f3bb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that a non-finetuned model could not have done this\n",
    "MODEL='gpt2'\n",
    "non_finetuned_latex_generator = pipeline(\n",
    "    'text-generation', \n",
    "    model=GPT2LMHeadModel.from_pretrained(MODEL),  # not fine-tuned!\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f3cb0b6-16f5-431a-985a-93c4a85bf94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: x to the fourth power\n",
      "LaTeX:\n",
      "LCT\n",
      "English: x to the fourth power\n",
      "LaTeX: x to the fifth power\n",
      "\n",
      "LaTeX: x to the sixth power\n"
     ]
    }
   ],
   "source": [
    "# Add our singular prompt\n",
    "CONVERSION_PROMPT = 'LCT\\n'  # LaTeX conversion task\n",
    "\n",
    "CONVERSION_TOKEN = 'LaTeX:'\n",
    "# text_sample = 'f of x is sum from 0 to x of x squared'\n",
    "text_sample='x to the fourth power'\n",
    "# text_sample = 'f of x equals x squared'\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(conversion_text_sample)\n",
    "print(non_finetuned_latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42ad3a-ad9f-4f4b-8f55-0530685757bd",
   "metadata": {},
   "source": [
    "## Dataset to train\n",
    "To finetune the model on this niche task. We need a dataset, here we will see the dataset and how we are preprocessing it for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3797380-ca2a-42fc-b718-3e20532faa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>LaTeX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>integral from a to b of x squared</td>\n",
       "      <td>\\int_{a}^{b} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>integral from negative 1 to 1 of x squared</td>\n",
       "      <td>\\int_{-1}^{1} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      English                   LaTeX\n",
       "0           integral from a to b of x squared   \\int_{a}^{b} x^2 \\,dx\n",
       "1  integral from negative 1 to 1 of x squared  \\int_{-1}^{1} x^2 \\,dx"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/english_to_latex.csv')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ec4b5-d97b-4599-a47a-2964375312ef",
   "metadata": {},
   "source": [
    "This is how the dataset is preprocessed for GPT to learn via prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8368f2a-4f52-4f55-840c-50edae606dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add our singular prompt\n",
    "CONVERSION_PROMPT = 'LCT\\n'  # LaTeX conversion task\n",
    "\n",
    "CONVERSION_TOKEN = 'LaTeX:'\n",
    "# This is our \"training prompt\" that we want GPT2 to recognize and learn\n",
    "training_examples = f'{CONVERSION_PROMPT}English: ' + data['English'] + '\\n' + CONVERSION_TOKEN + ' ' + data['LaTeX'].astype(str)\n",
    "\n",
    "print(training_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb779568-71ab-42f0-bb62-b9edf4e0fc8b",
   "metadata": {},
   "source": [
    "## Finetune\n",
    "Now we have the dataset preprocessed, we will now finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bf5f5f7-9d4a-4da5-9ad9-a31d5b216811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Preparing files to send to master... 10.5KB and 8 files\n",
      "Created experiment 60\n"
     ]
    }
   ],
   "source": [
    "!det experiment create \\\n",
    "    determined_files/const_eng_to_latex.yaml \\\n",
    "    determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c70a3d-1660-4a5c-9beb-886aab18fdf2",
   "metadata": {},
   "source": [
    "from determined.experimental import client\n",
    "# Lets see how trained checkpoint performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0aa4113-cd8b-4f5f-b0c2-1f8f36fe4142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30eab3c5-4ea7-48a5-9873-e09a2c785336\n"
     ]
    }
   ],
   "source": [
    "# Get the best checkpoint from the training\n",
    "experiment_id = 71\n",
    "MODEL_NAME = \"gpt2\"\n",
    "checkpoint = client.get_experiment(experiment_id).top_checkpoint(sort_by=\"eval_loss\", smaller_is_better=True)\n",
    "print(checkpoint.uuid)\n",
    "loaded_model = load_model_from_checkpoint(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6de4502-05f5-440c-b099-bda4f03ef31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latex_generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd50b1-3f62-4a94-b28a-5fe3dee26d14",
   "metadata": {},
   "source": [
    "# Extra: few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7e22a5a-d3d0-4db2-8ea4-f467fb337eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: x to the fourth power\n",
      "LaTeX: x^4 * \\pi^4 \\,dx\n",
      "LaTeX: x^4 * \\\n"
     ]
    }
   ],
   "source": [
    "# text_sample = 'f of x equals integral from 0 to pi of x to the fourth power'\n",
    "text_sample = 'x to the fourth power'\n",
    "\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "print(latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9251f2-6905-49d6-9b5f-8727e88431ae",
   "metadata": {},
   "source": [
    "# Lets see hyperparameter search at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5c719-3250-4b09-8e96-e06b4360a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create determined_files/adaptive_ds_chatbot.yaml determined_files/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550710a-377b-454a-a606-0dccf8ac6f69",
   "metadata": {},
   "source": [
    "# Extra knowledge to improve Generative Modeling at Inference time: Few Shot prompting\n",
    "Here we include examples of correct conversions for GPT for additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77d056a0-4fa2-49e8-abe1-2e344a5c47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"LCT\n",
    "English: f of x is sum from 0 to x of x squared\n",
    "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: f of x equals integral from 0 to pi of x to the fourth power\n",
    "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: f of x is x squared\n",
    "LaTeX:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f75d8e2d-e1de-46fd-bdb6-06e618332710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx ###\n",
      "LCT\n",
      "English: f of x equals integral from 0 to pi of x to the fourth power\n",
      "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx ###\n",
      "LCT\n",
      "English: f of x is x squared\n",
      "LaTeX: f(x) = \\int_{0}^{\\pi} x^2 \\,dx\n"
     ]
    }
   ],
   "source": [
    "print(latex_generator(\n",
    "    few_shot_prompt, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(few_shot_prompt)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77e299-2787-40bf-9c1e-b9711facbb31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# User Exercise 1: Finetune book to author of choice\n",
    "Lets form into groups, download text from online, and train our own chatbot!\n",
    "\n",
    "Steps to integrate custom dataset:\n",
    "* Go to project gutenburg and pick a book: (i.e. https://www.gutenberg.org/ebooks/1787 )\n",
    "* copy URL Plain Text UTF-8 .txt file and download using command: \n",
    "    - Example: `wget -O hamlet.txt https://www.gutenberg.org/cache/epub/1787/pg1787.txt`\n",
    "* move to shared directory: `cp hamlet.txt /run/determined/workdir/shared_fs/exercise/ -v`\n",
    "* Copy `run_det_ds_chatbot.sh` and rename: \n",
    "    - i.e. `cp determined_files/const_ds_chatbot.yaml determined_files/const_hamlet_chatbot.yaml `\n",
    "* NOTE: MAKE SURE THAT the file is a text file, and that there are no spaces in the name of the text file\n",
    "* Finally, change `name` field that describes the name of the experiment\n",
    "    - example -> `name: gpt2_finetune_hamlet_chatbot`\n",
    "* Change the `dataset_name` to name of text file: (i.e. hamlet)\n",
    "    - Do not include `.txt` in dataset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45962281-39fa-47ff-bd4c-f5caf5a683da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O <CHANGE_NAME>.txt <URL_TO_TXT_FILE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0799d07-c567-47af-a304-88665283f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "'hamlet.txt' -> '/run/determined/workdir/shared_fs/workshop_data/hamlet.txt'\n"
     ]
    }
   ],
   "source": [
    "!cp <CHANGE_NAME>.txt /run/determined/workdir/shared_fs/workshop_data/ -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa464f75-d6d3-4f51-a65a-522f77ae452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp determined_files/const_ds_chatbot.yaml determined_files/const_<CHANGE_NAME>_chatbot.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c273cc-3b81-4fad-8b5f-a90a31bfb9ab",
   "metadata": {},
   "source": [
    "Edit the following fields in `const_<CHANGE_NAME>_chatbot.yaml`\n",
    "```yaml\n",
    "name: <CHANGE_EXPERIMENT_NAME>\n",
    "workspace: <your_workspace>\n",
    "project: <your_project>\n",
    "description: \"Chicago DS Workshop\"\n",
    "hyperparameters:\n",
    "    global_batch_size: 32\n",
    "    weight_decay: 0.0\n",
    "    learning_rate: 5e-5\n",
    "    adam_epsilon: 1e-8\n",
    "    warmup_steps: 0\n",
    "    epochs: 10\n",
    "    device: 'cuda'\n",
    "    gradient_accumulation_steps: 1\n",
    "    dataset_name: <DATASET_NAME_TO_NEW_NAME>\n",
    "    train_batch_size: 32\n",
    "    eval_batch_size: 32\n",
    "environment:\n",
    "    image: \"hugcyrill/workshops:chat_0.1\"\n",
    "records_per_epoch: 147 # 4696 examples, 128 per batch: 601088/21 is 147 records for an epoch\n",
    "resources:\n",
    "    slots_per_trial: 1\n",
    "min_validation_period:\n",
    "  batches: 4\n",
    "min_checkpoint_period:\n",
    "  batches: 147\n",
    "searcher:\n",
    "    name: single\n",
    "    metric: eval_loss\n",
    "    max_length:\n",
    "        epochs: 30\n",
    "    smaller_is_better: true\n",
    "max_restarts: 0\n",
    "entrypoint: model_def:GPT2Finetune\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c3071bb-bcc2-4de5-a99f-11618561bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Preparing files to send to master... 19.9KB and 16 files\n",
      "Created experiment 74\n"
     ]
    }
   ],
   "source": [
    "# Run finetuning job \n",
    "!det experiment create determined_files/const_<CHANGE_ME>_chatbot.yaml determined_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a663627-a093-4612-a0e0-9df070cac145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce16c591-252a-4d22-8fb6-9258e8c01542\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "experiment_id = 74\n",
    "MODEL_NAME = \"gpt2\"\n",
    "checkpoint = client.get_experiment(experiment_id).top_checkpoint(sort_by=\"eval_loss\", smaller_is_better=True)\n",
    "print(checkpoint.uuid)\n",
    "loaded_model = load_model_from_checkpoint(checkpoint)\n",
    "\n",
    "finetuned_generator = pipeline(\n",
    "    'text-generation', model=loaded_model, tokenizer=tokenizer,\n",
    "    config={'max_length': 200,  'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "668be4fa-2528-4207-afaa-f39a324036a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='My name' # Example: PROMPT='Hamlet:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "049d69a3-eec8-4600-ab48-e310eab97dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------')\n",
    "for generated_sequence in finetuned_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c8f247-e0c6-4f5c-be5a-73447d3890ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
