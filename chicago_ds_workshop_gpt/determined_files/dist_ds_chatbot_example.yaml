name: dist_gpt2_finetune_data_science_chatbot
workspace: <your_workspace>
project: <your_project>
description: "Chicago DS Workshop"
hyperparameters:
    global_batch_size: 64
    weight_decay: 0.0
    learning_rate: 5e-5
    adam_epsilon: 1e-8
    warmup_steps: 0
    epochs: 10
    gradient_accumulation_steps: 1
    dataset_name: 'PDS2'
environment:
    image: "hugcyrill/workshops:chat_0.1"
    environment_variables:                                                                       
    - NCCL_DEBUG=INFO                                                                           
    # You may need to modify this to match your network configuration.                          
    - NCCL_SOCKET_IFNAME=ens,eth,ib
records_per_epoch: 147  # 4696 examples is the full amount, shortening to 147 for experimentation
resources:
    slots_per_trial: 2
min_validation_period:
  batches: 4
profiling:
  begin_on_batch: 0
  enabled: true
min_checkpoint_period:
  batches: 147
searcher:
    name: single
    metric: eval_loss
    max_length:
        epochs: 15
    smaller_is_better: true
max_restarts: 0
entrypoint: python3 -m determined.launch.torch_distributed --trial model_def:GPT2Finetune