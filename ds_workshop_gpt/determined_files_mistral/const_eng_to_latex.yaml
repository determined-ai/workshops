name: mistral_finetune_english2latex_chatbot
workspace: HPE Instructors
project: finetune
description: "DS Workshop"
hyperparameters:
    global_batch_size: 2
    weight_decay: 0.0
    learning_rate: 5e-5
    adam_epsilon: 1e-8
    warmup_steps: 0
    epochs: 10
    gradient_accumulation_steps: 1
    dataset_name: 'english_to_latex'
environment:
    image: "mendeza/mistral-rag-env:0.0.1"
records_per_epoch: 50 # 50 examples, 2 per batch: 50/2 is 25 records for an epoch
resources:
    slots_per_trial: 1
min_validation_period:
  batches: 2
profiling:
  begin_on_batch: 0
  enabled: true
min_checkpoint_period:
  batches: 50
searcher:
    name: single
    metric: eval_loss
    max_length:
        epochs: 3
    smaller_is_better: true
max_restarts: 0
entrypoint: python3 -m determined.launch.torch_distributed --trial model_def:MistralFinetune