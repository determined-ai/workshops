{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c0c5b6-0435-44d9-aedf-85adf923eb59",
   "metadata": {},
   "source": [
    "<img src=\"imgs/hpe_logo.png\" alt=\"HPE Logo\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c37a8-f4b6-45a3-ac61-cc8a0ac2962d",
   "metadata": {},
   "source": [
    "# Data Science Summit Workshop: \n",
    "## Creating chatbots by finetuning GPT models with Machine Learning Development Environment\n",
    " ----\n",
    "\n",
    "## Objective: Train your own chatbot\n",
    "This notebook walks you through finetuning your own chatbot.\n",
    "We will learn what are Generative Pretrained Transformers (GPT) and how you can finetune them for domain specific chatbots.\n",
    "\n",
    "## Motivation: The rise of generative language modeling\n",
    "Chatbots can be useful across many enterprise applications:\n",
    "* `Enterprise`: Chatbots for helpdesk support\n",
    "* `Healthcare`: Chatbots for scheduling appts, manage coverage, process claims\n",
    "* `Manufacturing`: Chatbots for checking supplies and inventory check\n",
    "* `Financial Services`: Chatbots for investment and account support\n",
    "\n",
    "Generative Language models like GPT-4 and ChatGPT enable exciting applications that were not possible previously! Unfortunately, enterprises can't use to use ChatGPT and GPT-4 if models need to analyze proprietary data. Here we show how to finetune open source GPT models for domain-specific applications and host on prem.\n",
    "\n",
    "The Machine Learning Development Environment can help data scientists and ML engineers finetune language models for enterprise usecases!\n",
    "\n",
    "## Why MLDE\n",
    "\n",
    "Developing robust, high performing Deep Learning (DL) application is challenging. To deploy DL applications succesfully great infrastructure. Building and managing distributed training, automatic checkpointing, hyperparameter search and metrics tracking is critical and challenging for small teams. \n",
    "\n",
    "Machine Learning Development Environment (MLDE) can remove the burden of writing and maintaining a custom training infrastructure and offers a streamlined approach to onboard new models to a state-of-the-art training platform, offering the following integrated platform features:\n",
    "\n",
    "<img src=\"imgs/det_components.jpg\" alt=\"Determined Components\" width=\"900\">\n",
    "\n",
    "MLDE provides a high-level framework APIs for PyTorch, Keras, and Estimators that let users describe their model without boilerplate code. MLDE reduces boilerplate by providing a state-of-the-art training loop that provides distributed training, hyperparameter search, automatic mixed precision, reproducibility, and many more features.\n",
    "\n",
    "## Overview of Workshop\n",
    "\n",
    "* Step 1: Overview what are GPT models\n",
    "* Step 2: Test a Data Science Chatbot using pretrained weights\n",
    "* Step 3: Dive into what model training looks like without Determined\n",
    "* Step 4: Overview of integrating Pytorch training code into MLDE\n",
    "* Step 5 - 6: Updating model configuration files, complete User Task\n",
    "* Step 7: Finetuning a  Chatbot on a Data Science Textbook\n",
    "* Step 8: Launch a distributed training Experiment\n",
    "* Step 9: Test a Chatbot to convert English text to Latex\n",
    "* Step 10: Explore and preprocess custom dataset to finetune Dataset to train\n",
    "* Step 11: Finetune Chatbot to convert English text to Latex\n",
    "* Step 12: Run inference on finetuned model\n",
    "* Step 13: (Optional): Improve inference with few shot prompting\n",
    "* User Exercise 1: Finetune chat book on book choice\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "This Demo is based on the following works: \n",
    "\n",
    "* https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "* https://github.com/sinanuozdemir/oreilly-transformers-video-series/blob/main/notebooks/8%20Hands_on_GPT.ipynb\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c271e3-0744-4c54-a4b9-dcd5dd9a4ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, GPT2LMHeadModel, pipeline, \\\n",
    "                         Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from determined.experimental import Determined\n",
    "from utils import load_model_from_checkpoint\n",
    "from determined.experimental import client\n",
    "from determined import pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c557e7d-3d7c-4629-9b0a-efa9b1effbd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 1: Overview What are GPT models\n",
    "\n",
    "<img src=\"imgs/openAI-gpt2.png\" alt=\"Determined Components\" width=\"900\">\n",
    "\n",
    "GPT2 is a transformer-based language model created by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.\n",
    "\n",
    "Since GPT2 was released, many larger GPT variants that are open sourced. Some examples available on huggingface include: GPT2-Medium, GPT2-Large and GPT2-XL.\n",
    "\n",
    "For a deeper dive into the model architecture, take a look at this  article: http://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a31e5b-8147-424f-8a5c-0c1367f47c6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 2: Test a Data Science Chatbot using pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b7537-1345-43d3-bef9-420a6b9266c5",
   "metadata": {},
   "source": [
    "We will load a pretrained model and see how it responds to data science questions. The model was pretrained on the WebText dataset (46 million urls, 40+GB of text)\n",
    "More info about the pretrained model can be view here: https://huggingface.co/docs/transformers/model_doc/gpt2\n",
    "\n",
    "Prompt we will run: `A test statistic is`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd68c9-e820-4872-8577-3a3ec23024d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # load up a GPT2 model\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', model=model, tokenizer='gpt2',\n",
    "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3cbc58-8870-4bf3-a39a-d86b868cab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='A test statistic is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f56d2-6208-4a7e-ab34-c16b41e2698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to see how model responds to prompt\n",
    "print('----------')\n",
    "for generated_sequence in pretrained_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3842cea-efb3-4938-ba7d-f62375eea2af",
   "metadata": {},
   "source": [
    "We can see the model can somewhat respond, but the response is for the most part not usable and nonsense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034676f4-6de4-46f9-a36a-786acb6ad2e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 3: Dive into what model training looks like without Determined\n",
    "If a data scientist were to write their own training code, this is what it would look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d945b-5674-4358-8ddb-fcdf7dd54ca1",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./data/PDS2.txt',  # Principles of Data Science - Sinan Ozdemir\n",
    "    block_size=32  # length of each chunk of text to use as a datapoint\n",
    ")\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "\n",
    "train_sampler = RandomSampler(dataset)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_dataloader = DataLoader(dataset, collate_fn =data_collator ,sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "t_total = len(dataset) // gradient_accumulation_steps * num_train_epochs\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.zero_grad()\n",
    "train_iterator = trange(int(num_train_epochs), desc=\"Epoch\", disable=local_rank not in [-1, 0])\n",
    "set_seed(0)\n",
    "for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=local_rank not in [-1, 0])\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        inputs, labels = (batch, batch) # batch contains a dict of {'labels', 'input_ids' and 'attention_mask'}\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        model.train()\n",
    "\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "        if fp16:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd299317-233a-4c5f-97b1-b033485736ec",
   "metadata": {},
   "source": [
    "Here is what it would look like to do hyperparameter search on the same training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392e2f2-eb61-4524-bc87-0d100ded0d1c",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def train(lr,m):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path='./data/PDS2.txt',  # Principles of Data Science - Sinan Ozdemir\n",
    "        block_size=32  # length of each chunk of text to use as a datapoint\n",
    "    )\n",
    "    config = GPT2Config.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model.to(device)\n",
    "    train_sampler = RandomSampler(dataset)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    train_dataloader = DataLoader(dataset, collate_fn =data_collator ,sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "    t_total = len(dataset) // gradient_accumulation_steps * num_train_epochs\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(num_train_epochs), desc=\"Epoch\", disable=local_rank not in [-1, 0])\n",
    "    set_seed(0)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            inputs, labels = (batch, batch) # batch contains a dict of {'labels', 'input_ids' and 'attention_mask'}\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "            if fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            loss.backward()\n",
    "    model, loss\n",
    "def hp_grid_search():\n",
    "    for lr in np.logspace(-4, -2, num=10):\n",
    "        for m in np.linspace(0.7, 0.95, num=10):\n",
    "            print(f\"Training model with learning rate {lr} and momentum {m}\")\n",
    "            model, loss = train(lr,m)\n",
    "            print(f\"Train Loss: {loss}\\n\")\n",
    "\n",
    "try:\n",
    "    hp_grid_search()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600006b-faae-4719-9e91-565e5db8e2e9",
   "metadata": {},
   "source": [
    "#### What's Missing?\n",
    "This approach works in theory -- we could get a good model, save it, and use it for predictions. But we're missing a lot from the ideal state:\n",
    "\n",
    "#### Distributed training\n",
    "    - Parallel search\n",
    "    - Intelligent checkpointing\n",
    "    - Interruptibility and fault tolerance\n",
    "    - Logging of experiment configurations and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2484ebd-c5dd-41c8-b80c-56bc7e5a48fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 4: Overview of integrating Pytorch training code into MLDE\n",
    "\n",
    "Here we will see how to implement the same training loop in MLDE, but automatically enable distributed training, automated checkpointing, and automatic hyperparameter search.\n",
    "\n",
    "The main components for any deep learning training loop are the following:\n",
    "* Datasets\n",
    "* Dataloader\n",
    "* Model\n",
    "* Optimizer\n",
    "* (Optional) Learn rate schedule\n",
    "* training a batch, evaluating a batch\n",
    "\n",
    "We will show how to integrate each core part into MLDE using the PyTorchTrial API. Note we have another API called CoreAPI, that supports flexibility if your team wants to integrate more complex Machine Learning codebases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951811f1-d9bf-4750-8599-4e9089919121",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Template Class that integrates DL code with MLDE\n",
    "The Template class is class we need to fill in to implement our training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afbaa0-0c2f-48be-a702-aef394a7670a",
   "metadata": {},
   "source": [
    "```python\n",
    "import filelock\n",
    "import os\n",
    "from typing import Any, Dict, Sequence, Tuple, Union, cast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from determined.pytorch import DataLoader, PyTorchTrial, PyTorchTrialContext\n",
    "\n",
    "import data\n",
    "\n",
    "TorchData = Union[Dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor]\n",
    "\n",
    "class GPT2Trial(PyTorchTrial):\n",
    "    def __init__(self, context: PyTorchTrialContext) -> None:\n",
    "        # Trial context contains info about the trial, such as the hyperparameters for training\n",
    "        self.context = context\n",
    "        \n",
    "        # init and wrap model, optimizer, LRscheduler, datasets\n",
    "       \n",
    "\n",
    "    def build_training_data_loader(self) -> DataLoader:\n",
    "        # create train dataloader from dataset\n",
    "        return DataLoader()\n",
    "\n",
    "    def build_validation_data_loader(self) -> DataLoader:\n",
    "        # create train dataloader from dataset\n",
    "        return DataLoader()\n",
    "\n",
    "    def train_batch(self, batch: TorchData, epoch_idx: int, batch_idx: int)  -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def evaluate_batch(self, batch: TorchData) -> Dict[str, Any]:\n",
    "        return {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182d059-603a-40ef-9d04-eab7d70eb884",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wrapping the Model\n",
    "* Wrapping model to the TrialContext allows MLDE to reduces boilerplate code\n",
    "* Providing a state-of-the-art training loop that provides distributed training, hyperparameter search, automatic mixed precision, reproducibility, and many more features\n",
    "* All the models, optimizers, and LR schedulers must be wrapped with wrap_model and wrap_optimizer respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7c6b2-1b6a-4347-a552-cfeb42169bd0",
   "metadata": {},
   "source": [
    "```python\n",
    "self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Wrapping model to the TrialContext \n",
    "self.model = self.context.wrap_model(self.model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1477398-8e18-4056-b1ce-398e48cd1d51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wrapping the Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4051af-bc83-43bb-acb0-b5529d402fe8",
   "metadata": {},
   "source": [
    "```python\n",
    "self.optimizer = self.context.wrap_optimizer(\n",
    "            AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.adam_epsilon)\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92578625-94e2-47eb-8304-aee0a00be52c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wrapping the Learn Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a5c51-d70c-46be-b976-4e5946c2c021",
   "metadata": {},
   "source": [
    "```python\n",
    "self.scheduler = self.context.wrap_lr_scheduler(\n",
    "    get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=self.warmup_steps,\n",
    "                                    num_training_steps=self.t_total),\n",
    "    LRScheduler.StepMode.MANUAL_STEP\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd085b7-6a66-4aeb-b1b6-cba5cc3bb126",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Integrating a Dataset\n",
    "Here we integrate the same TextDataset (used in Step 3) that formats and preprocess our text file to finetune our GPT model on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf815c-2c3e-44a4-bc1a-dfb84eee703b",
   "metadata": {},
   "source": [
    "```python\n",
    "dataset = TextDataset(\n",
    "                tokenizer=tokenizer,\n",
    "                file_path='/run/determined/workdir/shared_fs/workshop_data/PDS2.txt',\n",
    "                block_size=32  # length of each chunk of text to use as a datapoint\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed797aff-ce43-4492-8810-a1295a2c9f60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implement Train Dataloader and Validation Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0b04b-b4ab-4eb0-85e0-9f302b2149ae",
   "metadata": {},
   "source": [
    "```python\n",
    "def build_training_data_loader(self) -> None:\n",
    "    '''\n",
    "    '''\n",
    "    self.train_sampler = RandomSampler(self.dataset)\n",
    "    self.train_dataloader = DataLoader(self.dataset, collate_fn =self.data_collator ,sampler=self.train_sampler, batch_size=self.train_batch_size)\n",
    "    return self.train_dataloader\n",
    "def build_validation_data_loader(self) -> None:\n",
    "    '''\n",
    "    '''\n",
    "    self.eval_sampler = SequentialSampler(self.dataset)\n",
    "    self.validataion_dataloader = DataLoader(self.dataset,collate_fn =self.data_collator, sampler=self.eval_sampler, batch_size=self.eval_batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20951d8-a14f-45cc-81e2-2b39b0b3f608",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implement Train Batch and Evaluate Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ed9c9-7be5-47a8-a8c4-5f3c0b30e869",
   "metadata": {},
   "source": [
    "```python\n",
    "def train_batch(self,batch,epoch_idx, batch_idx):\n",
    "    '''\n",
    "    '''\n",
    "    inputs,labels = self.format_batch(batch)\n",
    "    outputs = self.model(inputs, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    train_result = {\n",
    "        'loss': loss\n",
    "    }\n",
    "    self.context.backward(train_result[\"loss\"])\n",
    "    self.context.step_optimizer(self.optimizer)\n",
    "    return train_result\n",
    "\n",
    "def evaluate_batch(self,batch):\n",
    "    '''\n",
    "    '''\n",
    "    inputs,labels = self.format_batch(batch)\n",
    "    outputs = self.model(inputs, labels=labels)\n",
    "    lm_loss = outputs[0]\n",
    "    eval_loss = lm_loss.mean().item()\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    results = {\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b0555-5cea-460d-b1e4-5b84ecc8180d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 5: Defining Training Experiment with Model Config\n",
    "In Determined, a trial is a training task that consists of a dataset, a deep learning model, and values for all of the model’s hyperparameters. An experiment is a collection of one or more trials: an experiment can either train a single model (with a single trial), or can define a search over a user-defined hyperparameter space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7016d230-c108-4cff-bbe6-be1caefd1b08",
   "metadata": {},
   "source": [
    "Here is what a configuration file looks like for a single trial experiment\n",
    "```yaml\n",
    "name: gpt2_finetune_data_science_chatbot\n",
    "workspace: <your_workspace>\n",
    "project: <your_project>\n",
    "description: \"DS Workshop\"\n",
    "hyperparameters:\n",
    "    global_batch_size: 32\n",
    "    weight_decay: 0.0\n",
    "    learning_rate: 5e-5\n",
    "    adam_epsilon: 1e-8\n",
    "    warmup_steps: 0\n",
    "    epochs: 10\n",
    "    gradient_accumulation_steps: 1\n",
    "    dataset_name: 'PDS2'\n",
    "environment:\n",
    "    image: \"hugcyrill/workshops:chat_0.1\"\n",
    "records_per_epoch: 147 # 4696 examples total, shortening for experimentation\n",
    "resources:\n",
    "    slots_per_trial: 1\n",
    "min_validation_period:\n",
    "  batches: 4\n",
    "min_checkpoint_period:\n",
    "  batches: 147\n",
    "searcher:\n",
    "    name: single\n",
    "    metric: eval_loss\n",
    "    max_length:\n",
    "        epochs: 30\n",
    "    smaller_is_better: true\n",
    "max_restarts: 0\n",
    "entrypoint: model_def:GPT2Finetune\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f523bbf-8a06-4ca8-8d4b-21760f60fd70",
   "metadata": {},
   "source": [
    "Here is what a configuration yaml file looks like to do a hyperparameter search\n",
    "```yaml\n",
    "name: adaptive_gpt2_finetune_data_science_chatbot\n",
    "workspace: <your_workspace>\n",
    "project: <your_project>\n",
    "description: \"DS Workshop\"\n",
    "hyperparameters:\n",
    "    global_batch_size: 32\n",
    "    weight_decay: 0.0\n",
    "    learning_rate:\n",
    "        type: log\n",
    "        minval: -6.0\n",
    "        maxval: -4.0\n",
    "        base: 10.0\n",
    "    adam_epsilon:\n",
    "        type: log\n",
    "        minval: -10.0\n",
    "        maxval: -4.0\n",
    "        base: 10.0\n",
    "    warmup_steps: 0\n",
    "    epochs: 10\n",
    "    gradient_accumulation_steps: 1\n",
    "    dataset_name: 'PDS2'\n",
    "environment:\n",
    "    image: \"hugcyrill/workshops:chat_0.1\"\n",
    "records_per_epoch: 147 # 4696 examples total, shortening for experimentation\n",
    "resources:\n",
    "    slots_per_trial: 1\n",
    "min_validation_period:\n",
    "  batches: 4\n",
    "min_checkpoint_period:\n",
    "  batches: 147\n",
    "searcher:\n",
    "    name: adaptive_asha\n",
    "    metric: eval_loss\n",
    "    max_length:\n",
    "        epochs: 30\n",
    "    smaller_is_better: true\n",
    "    max_trials: 4\n",
    "max_restarts: 0\n",
    "entrypoint: model_def:GPT2Finetune\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b75c6e6-0501-4621-973b-99d3f704cde0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 6: Updating model configuration files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43e3c7-06e3-491b-a505-bdf9f959b96c",
   "metadata": {},
   "source": [
    "## Step 6.1: Update `const_ds_chatbot.yaml`\n",
    "Please  update the `const_ds_chatbot.yaml` file in the `determined_files/` folder and look for the workspace and project fields. Replace the placeholders with your workspace name and project name, which you created during the preparation session:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa3bb8-2112-42ae-af95-53ba8a2f38f1",
   "metadata": {},
   "source": [
    "```yaml\n",
    "workspace: <your_workspace>\n",
    "project: <your_project>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffafc3d-7a7a-45d3-b5e9-31a4f31799a6",
   "metadata": {},
   "source": [
    "## Step 6.2: update `const_eng_to_latex.yaml`\n",
    "Update the `const_eng_to_latex.yaml` file in the `determined_files/`in the same manner.  Replace the placeholders with your workspace name and project name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86215-287d-47b8-8f16-d9942fd919cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 7: Finetuning a Chatbot on a Data Science Textbook\n",
    "Run cell to start finetuning chatbot on data science book. We have already configured our experiment in `determined_files/const_ds_chatbot.yaml` that trains on a text file called PDS2.txt. This is a text file from a Pakt publishing book called \"Principles of Data Science\". Link to the book here https://www.packtpub.com/product/principles-of-data-science/9781785887918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d407a-0cd3-44b3-a909-b44f00d7ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create \\\n",
    "    determined_files/const_ds_chatbot.yaml \\\n",
    "    determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32982e-834b-404d-ba3e-e6adc3bff3e5",
   "metadata": {},
   "source": [
    "### Step 7.1 See Result\n",
    "Replace the experiment ID with their ID once the experiment is completed in MLDE. only then they will be able to test their finetuned model on their book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54ab72-b109-4b9b-a877-ab6d4ca82435",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = <EXP_ID>\n",
    "MODEL_NAME = \"gpt2\"\n",
    "checkpoint = client.get_experiment(experiment_id).top_checkpoint(sort_by=\"eval_loss\", smaller_is_better=True)\n",
    "print(checkpoint.uuid)\n",
    "loaded_model = load_model_from_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2582a-50fd-43f6-a1d3-efd7fcfced6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_generator = pipeline(\n",
    "    'text-generation', model=loaded_model, tokenizer=tokenizer,\n",
    "    config={'max_length': 200,  'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a094cab-8b6c-4117-b08c-eb6066e394ba",
   "metadata": {},
   "source": [
    "Here is how the chatbot responds to same prompt after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd448dc-f45a-4d29-8405-93c75721e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='A test statistic is a value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d94e5-11f1-4bd7-9518-c59eeac7c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------')\n",
    "for generated_sequence in finetuned_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c79f8-e263-4959-9f24-daa506b08529",
   "metadata": {},
   "source": [
    "# Step 8: Launch a distributed training Experiment\n",
    "With Determined, to scale to a multi-GPU distributed training job only requires a single configuration line change. There is no need to worry about setting up frameworks like Horovod or PyTorch Lightning.\n",
    "\n",
    "Distributed changing to train a 2 gpu job is located at `dist_ds_chatbot.yaml`. Copy`const_ds_chatbot.yaml` and rename to `dist_ds_chatbot.yaml`. Change the <b>slots_per_trial field from 1 to 2</b> to run a distributed training job on 2 GPUs. The below cell we can execute the job. Make sure that the workspace and project is filled in:\n",
    "```yaml\n",
    "workspace: <your_workspace>\n",
    "project: <your_project>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f950191-5c4d-4f60-ab78-5d286f16a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create \\\n",
    "    determined_files/dist_ds_chatbot.yaml \\\n",
    "    determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e528c-a9db-4f28-a4d5-37115a39860d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 9: Test a Chatbot to convert English text to Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be3455-aec5-43de-ba08-186e3c1386ca",
   "metadata": {},
   "source": [
    "Here we are downloading pretrained weights and seeing how GPT does on a question to convert an english description into latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f14a9-2ce1-4453-94a4-fd9a5f3bb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='gpt2'\n",
    "non_finetuned_latex_generator = pipeline(\n",
    "    'text-generation', \n",
    "    model=GPT2LMHeadModel.from_pretrained(MODEL),  # not fine-tuned!\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cb0b6-16f5-431a-985a-93c4a85bf94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our singular prompt\n",
    "CONVERSION_PROMPT = 'LCT\\n'  # LaTeX conversion task\n",
    "\n",
    "CONVERSION_TOKEN = 'LaTeX:'\n",
    "\n",
    "text_sample='x to the fourth power'\n",
    "\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(conversion_text_sample)\n",
    "print(non_finetuned_latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42ad3a-ad9f-4f4b-8f55-0530685757bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 10: Explore and preprocess custom dataset to finetune Dataset to train\n",
    "To finetune the model on this niche task. We need a dataset, here we will see the dataset and how we are preprocessing it for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3797380-ca2a-42fc-b718-3e20532faa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/english_to_latex.csv')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ec4b5-d97b-4599-a47a-2964375312ef",
   "metadata": {},
   "source": [
    "This is how the dataset is preprocessed for GPT to learn via prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8368f2a-4f52-4f55-840c-50edae606dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add our singular prompt\n",
    "CONVERSION_PROMPT = 'LCT\\n'  # LaTeX conversion task\n",
    "\n",
    "CONVERSION_TOKEN = 'LaTeX:'\n",
    "# This is our \"training prompt\" that we want GPT2 to recognize and learn\n",
    "training_examples = f'{CONVERSION_PROMPT}English: ' + data['English'] + '\\n' + CONVERSION_TOKEN + ' ' + data['LaTeX'].astype(str)\n",
    "\n",
    "print(training_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb779568-71ab-42f0-bb62-b9edf4e0fc8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 11: Finetune Chatbot to convert English text to Latex\n",
    "Now we have the dataset preprocessed, we will now finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5f5f7-9d4a-4da5-9ad9-a31d5b216811",
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create \\\n",
    "    determined_files/const_eng_to_latex.yaml \\\n",
    "    determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c70a3d-1660-4a5c-9beb-886aab18fdf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 12: Run inference on finetuned model\n",
    "Replace the experiment ID with their ID once the experiment is completed in MLDE. only then they will be able to test their finetuned model on their book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa4113-cd8b-4f5f-b0c2-1f8f36fe4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best checkpoint from the training\n",
    "experiment_id = <EXP_ID>\n",
    "MODEL_NAME = \"gpt2\"\n",
    "checkpoint = client.get_experiment(experiment_id).top_checkpoint(sort_by=\"eval_loss\", smaller_is_better=True)\n",
    "print(checkpoint.uuid)\n",
    "loaded_model = load_model_from_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de4502-05f5-440c-b099-bda4f03ef31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e22a5a-d3d0-4db2-8ea4-f467fb337eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = 'x to the fourth power'\n",
    "\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "print(latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550710a-377b-454a-a606-0dccf8ac6f69",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 13 (Optional): Improve inference with few shot prompting\n",
    "Here we include examples of correct conversions for GPT for additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d056a0-4fa2-49e8-abe1-2e344a5c47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"LCT\n",
    "English: f of x is sum from 0 to x of x squared\n",
    "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: f of x equals integral from 0 to pi of x to the fourth power\n",
    "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: f of x is x to the third power\n",
    "LaTeX:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d8e2d-e1de-46fd-bdb6-06e618332710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latex_generator(\n",
    "    few_shot_prompt, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(few_shot_prompt)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77e299-2787-40bf-9c1e-b9711facbb31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# User Exercise 1: Finetune chat book on book choice\n",
    "Lets form into groups, download text from online, and train our own chatbot!\n",
    "\n",
    "Steps to integrate custom dataset:\n",
    "* Go to project gutenburg and pick a book: (i.e. https://www.gutenberg.org/ebooks/1787 )\n",
    "* copy URL Plain Text UTF-8 .txt file and download using command: \n",
    "    - Example: `wget -O hamlet.txt https://www.gutenberg.org/cache/epub/1787/pg1787.txt`\n",
    "* move to shared directory: `cp hamlet.txt /run/determined/workdir/shared_fs/exercise/ -v`\n",
    "* Copy `run_det_ds_chatbot.sh` and rename: \n",
    "    - i.e. `cp determined_files/const_ds_chatbot.yaml determined_files/const_hamlet_chatbot.yaml `\n",
    "* NOTE: MAKE SURE THAT the file is a text file, and that there are no spaces in the name of the text file\n",
    "* Finally, change `name` field that describes the name of the experiment\n",
    "    - example -> `name: gpt2_finetune_hamlet_chatbot`\n",
    "* Change the `dataset_name` to name of text file: (i.e. hamlet)\n",
    "    - Do not include `.txt` in dataset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45962281-39fa-47ff-bd4c-f5caf5a683da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O <CHANGE_NAME>.txt <URL_TO_TXT_FILE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0799d07-c567-47af-a304-88665283f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp <CHANGE_NAME>.txt /run/determined/workdir/shared_fs/workshop_data/ -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa464f75-d6d3-4f51-a65a-522f77ae452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp determined_files/const_ds_chatbot.yaml determined_files/const_<CHANGE_NAME>_chatbot.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c273cc-3b81-4fad-8b5f-a90a31bfb9ab",
   "metadata": {},
   "source": [
    "Edit the following fields in `const_<CHANGE_NAME>_chatbot.yaml`\n",
    "```yaml\n",
    "name: <CHANGE_EXPERIMENT_NAME>\n",
    "workspace: <your_workspace>\n",
    "project: <your_project>\n",
    "description: \"DS Workshop\"\n",
    "hyperparameters:\n",
    "    global_batch_size: 32\n",
    "    weight_decay: 0.0\n",
    "    learning_rate: 5e-5\n",
    "    adam_epsilon: 1e-8\n",
    "    warmup_steps: 0\n",
    "    epochs: 10\n",
    "    device: 'cuda'\n",
    "    gradient_accumulation_steps: 1\n",
    "    dataset_name: <DATASET_NAME_TO_NEW_NAME>\n",
    "    train_batch_size: 32\n",
    "    eval_batch_size: 32\n",
    "environment:\n",
    "    image: \"hugcyrill/workshops:chat_0.1\"\n",
    "records_per_epoch: 147 # 4696 examples, 128 per batch: 601088/21 is 147 records for an epoch\n",
    "resources:\n",
    "    slots_per_trial: 1\n",
    "min_validation_period:\n",
    "  batches: 4\n",
    "min_checkpoint_period:\n",
    "  batches: 147\n",
    "searcher:\n",
    "    name: single\n",
    "    metric: eval_loss\n",
    "    max_length:\n",
    "        epochs: 30\n",
    "    smaller_is_better: true\n",
    "max_restarts: 0\n",
    "entrypoint: model_def:GPT2Finetune\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3071bb-bcc2-4de5-a99f-11618561bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run finetuning job \n",
    "!det experiment create determined_files/const_<CHANGE_ME>_chatbot.yaml determined_files/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f9bfc-c079-4ed5-8e4f-1bc90e580a0b",
   "metadata": {},
   "source": [
    "Replace the experiment ID with their ID once the experiment is completed in MLDE. only then they will be able to test their finetuned model on their book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a663627-a093-4612-a0e0-9df070cac145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "experiment_id = <EXP_ID>\n",
    "MODEL_NAME = \"gpt2\"\n",
    "checkpoint = client.get_experiment(experiment_id).top_checkpoint(sort_by=\"eval_loss\", smaller_is_better=True)\n",
    "print(checkpoint.uuid)\n",
    "loaded_model = load_model_from_checkpoint(checkpoint)\n",
    "\n",
    "finetuned_generator = pipeline(\n",
    "    'text-generation', model=loaded_model, tokenizer=tokenizer,\n",
    "    config={'max_length': 200,  'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668be4fa-2528-4207-afaa-f39a324036a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='My name' # Example: PROMPT='Hamlet:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d69a3-eec8-4600-ab48-e310eab97dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------')\n",
    "for generated_sequence in finetuned_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
