name: opt125m_adaptive_finetune
workspace: HPE Instructors
project: finetune
description: "OPT125m"
hyperparameters:
    global_batch_size: 32
    weight_decay: 0.0
    learning_rate:
        type: log
        minval: -6.0
        maxval: -4.0
        base: 10.0
    adam_epsilon:
        type: log
        minval: -10.0
        maxval: -8.0
        base: 10.0
    warmup_steps: 0
    epochs: 10
    gradient_accumulation_steps: 1
    dataset_name: 'PDS2'
bind_mounts:
  - container_path: /mnt/efs
    host_path: /mnt/efs
    propagation: rprivate
    read_only: false
environment:
    image: "mendeza/mistral-rag-env:0.0.1"
records_per_epoch: 64 # There are 4696 examples total in the PDS2 dataset, shortening to 147 records per epoch for experimentation
resources:
    slots_per_trial: 2
    resource_pool: compute-pool-a10
min_validation_period:
  batches: 2
profiling:
  begin_on_batch: 0
  enabled: true
min_checkpoint_period:
  batches: 30
searcher:
    name: adaptive_asha
    metric: eval_loss
    max_length:
        epochs: 30
    smaller_is_better: true
    max_trials: 4
max_restarts: 0
entrypoint: python3 -m determined.launch.torch_distributed --trial model_def:OPTFinetuneTrial

